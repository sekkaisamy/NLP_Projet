{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv files\n",
    "df1 = pd.read_csv(\"dailydialog.csv\")\n",
    "df2 = pd.read_csv(\"emotion-stimulus.csv\")\n",
    "df3 = pd.read_csv(\"isear.csv\")\n",
    "df4 = pd.read_csv(\"emotion_dataset.csv\")\n",
    "df5 = pd.read_csv(\"emotion_dataset2.csv\")\n",
    "df6 = pd.read_csv(\"emotion-tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames row-wise\n",
    "df = pd.concat([df1, df2, df3,df4,df5,df6], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral      87826\n",
       "joy          20503\n",
       "sadness      15119\n",
       "Happiness    12885\n",
       "fear         11539\n",
       "anger        10261\n",
       "surprise      4994\n",
       "disgust       2018\n",
       "Surpise       1823\n",
       "shame         1346\n",
       "love          1304\n",
       "Sadness       1150\n",
       "guilt         1052\n",
       "Anger         1022\n",
       "sad            575\n",
       "happy          479\n",
       "Disgust        353\n",
       "love           337\n",
       "Fear           174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the count of all values\n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check for duplicate rows\\nduplicates = df[\\'Emotion\\'].duplicated()\\n\\n# Print the rows that are duplicates\\nprint(df[duplicates])\\n\\n# Count the number of duplicate rows\\nnum_duplicates = duplicates.sum()\\nprint(f\"Number of duplicate rows: {num_duplicates}\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Check for duplicate rows\n",
    "duplicates = df['Emotion'].duplicated()\n",
    "\n",
    "# Print the rows that are duplicates\n",
    "print(df[duplicates])\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Remove duplicate rows\\ndf = df.drop_duplicates()\\n\\n# Reset the index after removing duplicates (optional)\\ndf = df.reset_index(drop=True)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "df = df.reset_index(drop=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check for duplicate rows\\nduplicates = df.duplicated()\\n\\n# Print the rows that are duplicates\\nprint(df[duplicates])\\n\\n# Count the number of duplicate rows\\nnum_duplicates = duplicates.sum()\\nprint(f\"Number of duplicate rows: {num_duplicates}\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "# Print the rows that are duplicates\n",
    "print(df[duplicates])\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral      87826\n",
       "joy          20503\n",
       "sadness      15119\n",
       "Happiness    12885\n",
       "fear         11539\n",
       "anger        10261\n",
       "surprise      4994\n",
       "disgust       2018\n",
       "Surpise       1823\n",
       "shame         1346\n",
       "love          1304\n",
       "Sadness       1150\n",
       "guilt         1052\n",
       "Anger         1022\n",
       "sad            575\n",
       "happy          479\n",
       "Disgust        353\n",
       "love           337\n",
       "Fear           174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the count of all values\n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only 6 labels to simplify\n",
    "df = df[~df['Emotion'].str.contains('love', case=False, na=False)]\n",
    "df = df[~df['Emotion'].str.contains('shame', case=False, na=False)]\n",
    "df = df[~df['Emotion'].str.contains('disgust', case=False, na=False)]\n",
    "df = df[~df['Emotion'].str.contains('guilt', case=False, na=False)]\n",
    "df = df[~df['Emotion'].str.contains('Surpise', case=False, na=False)]\n",
    "df = df[~df['Emotion'].str.contains('surprise', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral      87826\n",
       "joy          20503\n",
       "sadness      15119\n",
       "Happiness    12885\n",
       "fear         11539\n",
       "anger        10261\n",
       "Sadness       1150\n",
       "Anger         1022\n",
       "sad            575\n",
       "happy          479\n",
       "Fear           174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the count of all values\n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "Emotion    0\n",
      "Text       0\n",
      "dtype: int64\n",
      "\n",
      "Number of null values per column:\n",
      "Emotion    0\n",
      "Text       0\n",
      "dtype: int64\n",
      "\n",
      "Total number of NaN values in the dataset: 0\n",
      "Total number of null values in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the NaN values in each column\n",
    "nan_counts = df.isna().sum()\n",
    "print(\"Number of NaN values per column:\")\n",
    "print(nan_counts)\n",
    "\n",
    "# Count the null values in each column (similar to isna(), but shown for illustration)\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"\\nNumber of null values per column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Display the total number of NaN values in the dataset\n",
    "total_nan_counts = df.isna().sum().sum()\n",
    "print(f'\\nTotal number of NaN values in the dataset: {total_nan_counts}')\n",
    "\n",
    "# Display the total number of null values in the dataset\n",
    "total_null_counts = df.isnull().sum().sum()\n",
    "print(f'Total number of null values in the dataset: {total_null_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Remove duplicate rows\\ndf = df.drop_duplicates()\\n\\n# Reset the index after removing duplicates (optional)\\ndf = df.reset_index(drop=True)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "df = df.reset_index(drop=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check for duplicate rows\\nduplicates = df.duplicated()\\n\\n# Print the rows that are duplicates\\nprint(df[duplicates])\\n\\n# Count the number of duplicate rows\\nnum_duplicates = duplicates.sum()\\nprint(f\"Number of duplicate rows: {num_duplicates}\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "# Print the rows that are duplicates\n",
    "print(df[duplicates])\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Emotion' column before mapping:\n",
      "['neutral' 'Happiness' 'Fear' 'Sadness' 'Anger' 'happy' 'sad' 'anger'\n",
      " 'fear' 'joy' 'sadness']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in 'Emotion' column before mapping:\")\n",
    "print(df['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels were successfully normalized.\n",
      "Unique labels after normalization:\n",
      "['neutral' 'Happiness' 'fear' 'sadness' 'anger' 'joy']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a mapping to unify redundant labels\n",
    "label_mapping = {\n",
    "    'Happiness': 'Happiness',\n",
    "    'happy': 'Happiness',\n",
    "    'Surprise': 'surprise',\n",
    "    'surprise': 'surprise',\n",
    "    'joy' :'joy',\n",
    "    'Sadness': 'sadness',\n",
    "    'sad': 'sadness',\n",
    "    'sadness': 'sadness',\n",
    "    'Fear': 'fear',\n",
    "    'fear': 'fear',\n",
    "    'Anger': 'anger',\n",
    "    'anger': 'anger',\n",
    "    'Disgust': 'disgust',\n",
    "    'disgust': 'disgust',\n",
    "    'shame': 'shame',\n",
    "    'guilt': 'guilt',\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Emotion' column\n",
    "df['Emotion'] = df['Emotion'].map(label_mapping)\n",
    "\n",
    "# Check for unmapped labels (if any)\n",
    "unmapped = df[df['Emotion'].isnull()]['Emotion'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"Unmapped labels: {unmapped}\")\n",
    "else:\n",
    "    print(\"All labels were successfully normalized.\")\n",
    "\n",
    "# Check the unique labels after normalization\n",
    "print(\"Unique labels after normalization:\")\n",
    "print(df['Emotion'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral      87826\n",
       "joy          20503\n",
       "sadness      16844\n",
       "Happiness    13364\n",
       "fear         11713\n",
       "anger        11283\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the count of all values of emotion data\n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "Emotion    0\n",
      "Text       0\n",
      "dtype: int64\n",
      "\n",
      "Number of null values per column:\n",
      "Emotion    0\n",
      "Text       0\n",
      "dtype: int64\n",
      "\n",
      "Total number of NaN values in the dataset: 0\n",
      "Total number of null values in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the NaN values in each column\n",
    "nan_counts = df.isna().sum()\n",
    "print(\"Number of NaN values per column:\")\n",
    "print(nan_counts)\n",
    "\n",
    "# Count the null values in each column (similar to isna(), but shown for illustration)\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"\\nNumber of null values per column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Display the total number of NaN values in the dataset\n",
    "total_nan_counts = df.isna().sum().sum()\n",
    "print(f'\\nTotal number of NaN values in the dataset: {total_nan_counts}')\n",
    "\n",
    "# Display the total number of null values in the dataset\n",
    "total_null_counts = df.isnull().sum().sum()\n",
    "print(f'Total number of null values in the dataset: {total_null_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Drop rows where 'Emotion' has NaN values\\ndf = df.dropna(subset=['Emotion'])\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Drop rows where 'Emotion' has NaN values\n",
    "df = df.dropna(subset=['Emotion'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral      87826\n",
       "joy          20503\n",
       "sadness      16844\n",
       "Happiness    13364\n",
       "fear         11713\n",
       "anger        11283\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the count of all values of emotion data\n",
    "df['Emotion'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Say , Jim , how about going for a few beers af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>You know that is tempting but is really not g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>What do you mean ? It will help us to relax .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Do you really think so ? I don't . It will ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I guess you are right.But what shall we do ? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral  Say , Jim , how about going for a few beers af...\n",
       "1  neutral   You know that is tempting but is really not g...\n",
       "2  neutral     What do you mean ? It will help us to relax . \n",
       "3  neutral   Do you really think so ? I don't . It will ju...\n",
       "4  neutral   I guess you are right.But what shall we do ? ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "Emotion    0\n",
      "Text       0\n",
      "dtype: int64\n",
      "\n",
      "Number of null values per column:\n",
      "Emotion    0\n",
      "Text       0\n",
      "dtype: int64\n",
      "\n",
      "Total number of NaN values in the dataset: 0\n",
      "Total number of null values in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the NaN values in each column\n",
    "nan_counts = df.isna().sum()\n",
    "print(\"Number of NaN values per column:\")\n",
    "print(nan_counts)\n",
    "\n",
    "# Count the null values in each column (similar to isna(), but shown for illustration)\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"\\nNumber of null values per column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Display the total number of NaN values in the dataset\n",
    "total_nan_counts = df.isna().sum().sum()\n",
    "print(f'\\nTotal number of NaN values in the dataset: {total_nan_counts}')\n",
    "\n",
    "# Display the total number of null values in the dataset\n",
    "total_null_counts = df.isnull().sum().sum()\n",
    "print(f'Total number of null values in the dataset: {total_null_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of 'neutral' comments: 87826\n",
      "Current number of 'Happiness' comments: 13364\n",
      "Current number of 'joy' comments: 20503\n",
      "Current number of 'sadness' comments: 16844\n",
      "Current number of 'fear' comments: 11713\n",
      "Current number of 'anger' comments: 11283\n",
      "Emotion\n",
      "Happiness    11000\n",
      "sadness      11000\n",
      "anger        11000\n",
      "joy          11000\n",
      "fear         11000\n",
      "neutral      11000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to sample a specific number of entries for each emotion\n",
    "def sample_emotions(df, emotions_to_sample, desired_count):\n",
    "    sampled_dfs = []\n",
    "    for emotion in emotions_to_sample:\n",
    "        # Filter entries for the specified emotion\n",
    "        emotion_comments = df[df['Emotion'] == emotion]\n",
    "        current_count = len(emotion_comments)\n",
    "        print(f\"Current number of '{emotion}' comments: {current_count}\")\n",
    "\n",
    "        # Check if there are enough entries to sample\n",
    "        if desired_count > current_count:\n",
    "            raise ValueError(f\"Desired count for '{emotion}' exceeds available entries.\")\n",
    "\n",
    "        # Randomly sample the desired number of entries\n",
    "        sampled_dfs.append(emotion_comments.sample(n=desired_count, random_state=42))\n",
    "    return pd.concat(sampled_dfs)\n",
    "\n",
    "# List of emotions to sample and desired count\n",
    "emotions_to_sample = ['neutral','Happiness','joy','sadness','fear','anger']\n",
    "desired_count = 11000\n",
    "\n",
    "# Sample the specified number of entries for each emotion\n",
    "sampled_emotions = sample_emotions(df, emotions_to_sample, desired_count)\n",
    "\n",
    "# Select all entries that are not in the sampled emotions\n",
    "other_comments = df[~df['Emotion'].isin(emotions_to_sample)]\n",
    "\n",
    "# Combine the sampled entries with the other comments\n",
    "df = pd.concat([sampled_emotions, other_comments])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new class distribution\n",
    "print(df['Emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation:\n",
      " Emotion\n",
      "Happiness    11000\n",
      "sadness      11000\n",
      "anger        11000\n",
      "joy          11000\n",
      "fear         11000\n",
      "neutral      11000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display class distribution\n",
    "class_counts = df['Emotion'].value_counts()\n",
    "print(\"Class distribution before augmentation:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after augmentation:\n",
      " Emotion\n",
      "Happiness    17000\n",
      "sadness      17000\n",
      "anger        17000\n",
      "joy          17000\n",
      "fear         17000\n",
      "neutral      17000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Initialize the Synonym-based augmenter\n",
    "augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Function to augment text using SynonymAug\n",
    "def augment_text_synonym(text, augmenter, n=1):\n",
    "    augmented_texts = [augmenter.augment(text) for _ in range(n)]\n",
    "    return augmented_texts\n",
    "\n",
    "# Set the minimum target count per class\n",
    "minimum_target_count = 17000\n",
    "\n",
    "# List to hold augmented data\n",
    "augmented_data = []\n",
    "\n",
    "# Augment data for each class\n",
    "for emotion, count in class_counts.items():\n",
    "    # Determine the number of samples to augment\n",
    "    num_augmentations = max(0, minimum_target_count - count)  # Ensure it's non-negative\n",
    "\n",
    "    if num_augmentations > 0:\n",
    "        # Filter the DataFrame for the current emotion\n",
    "        df_emotion = df[df['Emotion'] == emotion]\n",
    "\n",
    "        while num_augmentations > 0:\n",
    "            for _, row in df_emotion.iterrows():\n",
    "                # Stop if we've reached the target\n",
    "                if num_augmentations <= 0:\n",
    "                    break\n",
    "\n",
    "                # Generate augmented texts\n",
    "                augmented_texts = augment_text_synonym(row['Text'], augmenter, n=1)  # Generate one at a time\n",
    "\n",
    "                for aug_text in augmented_texts:\n",
    "                    # Ensure `aug_text` is a string\n",
    "                    if isinstance(aug_text, list):\n",
    "                        aug_text = aug_text[0]  # Extract the string from the list\n",
    "                    augmented_data.append({'Text': aug_text, 'Emotion': emotion})\n",
    "                    num_augmentations -= 1  # Reduce the count needed\n",
    "                    if num_augmentations <= 0:\n",
    "                        break\n",
    "\n",
    "# Create a DataFrame for the augmented data\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Combine the original DataFrame with the augmented data\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "# Display class distribution after augmentation\n",
    "class_counts_after_augmentation = df['Emotion'].value_counts()\n",
    "print(\"Class distribution after augmentation:\\n\", class_counts_after_augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check for duplicate rows\\nduplicates = df.duplicated()\\n\\n# Print the rows that are duplicates\\nprint(\"Duplicate rows:\")\\nprint(df[duplicates])\\n\\n# Count the number of duplicate rows\\nnum_duplicates = duplicates.sum()\\nprint(f\"Number of duplicate rows: {num_duplicates}\")'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "# Print the rows that are duplicates\n",
    "print(\"Duplicate rows:\")\n",
    "print(df[duplicates])\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Remove duplicate rows\\ndf = df.drop_duplicates()\\n\\n# Reset the index after removing duplicates (optional)\\ndf = df.reset_index(drop=True)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "df = df.reset_index(drop=True)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Display class distribution\\nclass_counts = df[\\'Emotion\\'].value_counts()\\nprint(\"Class distribution before augmentation:\\n\", class_counts)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Display class distribution\n",
    "class_counts = df['Emotion'].value_counts()\n",
    "print(\"Class distribution before augmentation:\\n\", class_counts)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check for duplicate rows\\nduplicates = df.duplicated()\\n\\n# Print the rows that are duplicates\\nprint(\"Duplicate rows:\")\\nprint(df[duplicates])\\n\\n# Count the number of duplicate rows\\nnum_duplicates = duplicates.sum()\\nprint(f\"Number of duplicate rows: {num_duplicates}\")'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "# Print the rows that are duplicates\n",
    "print(\"Duplicate rows:\")\n",
    "print(df[duplicates])\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the augmented dataset\n",
    "df.to_csv('augmented_dataset_dialog_samy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions were successfully mapped to sentiments.\n",
      "     Emotion                                               Text Sentiment\n",
      "0  Happiness                                      Yes . When ?   positive\n",
      "1    sadness  I don't think I could cope with your heartbrea...  negative\n",
      "2    sadness                 Lundi y a J Cole je n'y serai pas   negative\n",
      "3  Happiness   It ’ s just the cows that are grazing over th...  positive\n",
      "4      anger  @MagicAndFangs 'Just by getting lost! I don't ...  negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping emotions to sentiments\n",
    "sentiment_map = {\n",
    "    'neutral': 'neutral',\n",
    "    'Happiness': 'positive',\n",
    "    'surprise': 'positive',\n",
    "    'joy' : 'positive',\n",
    "    'sadness': 'negative',\n",
    "    'anger': 'negative',\n",
    "    'fear': 'negative',\n",
    "    'disgust': 'negative',\n",
    "    'shame': 'negative',\n",
    "    'guilt': 'negative'\n",
    "    \n",
    "}\n",
    "\n",
    "# Create a new column 'Sentiment' by mapping the 'Emotion' column\n",
    "df['Sentiment'] = df['Emotion'].map(sentiment_map)\n",
    "\n",
    "# Check for unmapped emotions (optional debugging step)\n",
    "unmapped = df[df['Sentiment'].isnull()]['Emotion'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"Unmapped emotions: {unmapped}\")\n",
    "else:\n",
    "    print(\"All emotions were successfully mapped to sentiments.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions were successfully mapped.\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping for emotions to unique integer codes\n",
    "emotion_mapping = {\n",
    "    'fear': 0,\n",
    "    'Happiness': 1,\n",
    "    'anger': 2,\n",
    "    'joy' : 3,   \n",
    "    'sadness': 4,    \n",
    "    'neutral': 5, \n",
    "}\n",
    "\n",
    "# Map the 'Emotion' column to integer codes\n",
    "df['Emotion'] = df['Emotion'].map(emotion_mapping)\n",
    "\n",
    "# Check for unmapped values in Emotion\n",
    "unmapped_emotions = df[df['Emotion'].isnull()]['Emotion'].unique()\n",
    "if len(unmapped_emotions) > 0:\n",
    "    print(f\"Unmapped emotions: {unmapped_emotions}\")\n",
    "else:\n",
    "    print(\"All emotions were successfully mapped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sentiments were successfully mapped.\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping for sentiments to unique integer codes\n",
    "sentiment_mapping = {\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "    'negative': 2\n",
    "}\n",
    "\n",
    "# Map the 'Sentiment' column to integer codes\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Check for unmapped values in Sentiment\n",
    "unmapped_sentiments = df[df['Sentiment'].isnull()]['Sentiment'].unique()\n",
    "if len(unmapped_sentiments) > 0:\n",
    "    print(f\"Unmapped sentiments: {unmapped_sentiments}\")\n",
    "else:\n",
    "    print(\"All sentiments were successfully mapped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "Emotion      0\n",
      "Text         0\n",
      "Sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "Number of null values per column:\n",
      "Emotion      0\n",
      "Text         0\n",
      "Sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "Total number of NaN values in the dataset: 0\n",
      "Total number of null values in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the NaN values in each column\n",
    "nan_counts = df.isna().sum()\n",
    "print(\"Number of NaN values per column:\")\n",
    "print(nan_counts)\n",
    "\n",
    "# Count the null values in each column (similar to isna(), but shown for illustration)\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"\\nNumber of null values per column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Display the total number of NaN values in the dataset\n",
    "total_nan_counts = df.isna().sum().sum()\n",
    "print(f'\\nTotal number of NaN values in the dataset: {total_nan_counts}')\n",
    "\n",
    "# Display the total number of null values in the dataset\n",
    "total_null_counts = df.isnull().sum().sum()\n",
    "print(f'Total number of null values in the dataset: {total_null_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes . When ?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't think I could cope with your heartbrea...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lundi y a J Cole je n'y serai pas</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It ’ s just the cows that are grazing over th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@MagicAndFangs 'Just by getting lost! I don't ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion  Sentiment\n",
       "0                                      Yes . When ?         1          1\n",
       "1  I don't think I could cope with your heartbrea...        4          2\n",
       "2                 Lundi y a J Cole je n'y serai pas         4          2\n",
       "3   It ’ s just the cows that are grazing over th...        1          1\n",
       "4  @MagicAndFangs 'Just by getting lost! I don't ...        2          2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder the columns\n",
    "df = df[['Text', 'Emotion', 'Sentiment']]\n",
    "# Display the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if text is a string (to avoid errors with NaNs)\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", '', text)\n",
    "        \n",
    "        # Remove hashtags and @usernames\n",
    "        text = re.sub(r\"(#[\\d\\w\\.]+)\", '', text)\n",
    "        text = re.sub(r\"(@[\\d\\w\\.]+)\", '', text)\n",
    "        \n",
    "        # Remove numbers (optional if numbers are relevant)\n",
    "        text = re.sub(r\"\\d+\", '', text)\n",
    "        \n",
    "        # **Remove underscores**\n",
    "        text = text.replace('_', '')\n",
    "        \n",
    "        # Remove all special characters except valid punctuation (if needed)\n",
    "        text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "        \n",
    "        # Remove non-ASCII characters\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", '', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "\n",
    "        # Tokenization using nltk\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words (if you have a defined stop words list)\n",
    "        # Uncomment and define stop words if needed:\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Return the cleaned and tokenized text\n",
    "        return tokens\n",
    "    \n",
    "    return text  # Return the input as is if it's not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the 'Text' column\n",
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the bclean augmented dataset\n",
    "df.to_csv('cleaned_augmented_dataset_dialog_samy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[yes, when]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, dont, think, i, could, cope, with, your, h...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[lundi, y, a, j, cole, je, ny, serai, pas]</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[it, s, just, the, cows, that, are, grazing, o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[just, by, getting, lost, i, dont, want, to, s...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[unbeknown, to, me, my, flatmate, has, become,...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[i, feel, will, be, warmly, welcomed, on, any,...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[im, going, to, do, some, shopping, why, dont,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[i, have, lemonade, iced, tea, and, mango, juice]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[when, a, republican, says, shining, city, i, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion  Sentiment\n",
       "0                                        [yes, when]        1          1\n",
       "1  [i, dont, think, i, could, cope, with, your, h...        4          2\n",
       "2         [lundi, y, a, j, cole, je, ny, serai, pas]        4          2\n",
       "3  [it, s, just, the, cows, that, are, grazing, o...        1          1\n",
       "4  [just, by, getting, lost, i, dont, want, to, s...        2          2\n",
       "5  [unbeknown, to, me, my, flatmate, has, become,...        3          1\n",
       "6  [i, feel, will, be, warmly, welcomed, on, any,...        3          1\n",
       "7  [im, going, to, do, some, shopping, why, dont,...        1          1\n",
       "8  [i, have, lemonade, iced, tea, and, mango, juice]        1          1\n",
       "9  [when, a, republican, says, shining, city, i, ...        0          2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'sorry', 'i', 'didnt', 'quite', 'catch', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(df['Text'][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract 'Emotion' and 'Sentiment' as labels\n",
    "y = df[['Emotion', 'Sentiment']]  \n",
    "\n",
    "# Extract 'Text' as features\n",
    "X = df['Text']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80% for training and 20% for temporary set (which will be split into validation and test sets)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the 20% temporary set into 10% validation and 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer,RobertaTokenizer\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Extract word index and vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "#Convert text into numerical values\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "#Calculate the max lengh of text\n",
    "max_length = 0\n",
    "for sequence in X_train:\n",
    "    sequence_length = len(sequence)\n",
    "    if sequence_length > max_length:\n",
    "        max_length = sequence_length\n",
    "\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding zeros at the beginning\n",
    "X_train = pad_sequences(X_train,maxlen=max_length)\n",
    "X_val = pad_sequences(X_val,maxlen=max_length)\n",
    "X_test = pad_sequences(X_test,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45035"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the max vocabulary size\n",
    "vocab_size = len(word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.0, 1: 1.0, 2: 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Calculate balanced class weights to handle class imbalance in a dataset.\n",
    "class_weights_sent = compute_class_weight(class_weight='balanced', classes=np.unique(y['Sentiment']), y=y['Sentiment'])\n",
    "\n",
    "# Convert weights to a dictionary format\n",
    "class_weight_dict_sent = dict(enumerate(class_weights_sent))\n",
    "\n",
    "# Print the class weight dictionary\n",
    "print(class_weight_dict_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate balanced class weights to handle class imbalance in a dataset.\n",
    "class_weights_emo = compute_class_weight(class_weight='balanced', classes=np.unique(y['Emotion']), y=y['Emotion'])\n",
    "\n",
    "# Convert weights to a dictionary format\n",
    "class_weight_dict_emo = dict(enumerate(class_weights_emo))\n",
    "\n",
    "# Print the class weight dictionary\n",
    "print(class_weight_dict_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to a NumPy array if it's a pandas Series\n",
    "y_train_sent = y_train['Sentiment'].values if isinstance(y_train['Sentiment'], pd.Series) else y_train['Sentiment']\n",
    "\n",
    "# Convert y_val to a NumPy array if it's a pandas Series\n",
    "y_val_sent = y_val['Sentiment'].values if isinstance(y_val['Sentiment'], pd.Series) else y_val['Sentiment']\n",
    "\n",
    "# Convert y_test to a NumPy array if it's a pandas Series\n",
    "y_test_sent = y_test['Sentiment'].values if isinstance(y_test['Sentiment'], pd.Series) else y_test['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to a NumPy array if it's a pandas Series\n",
    "y_train_emo = y_train['Emotion'].values if isinstance(y_train['Emotion'], pd.Series) else y_train['Emotion']\n",
    "\n",
    "# Convert y_val to a NumPy array if it's a pandas Series\n",
    "y_val_emo = y_val['Emotion'].values if isinstance(y_val['Emotion'], pd.Series) else y_val['Emotion']\n",
    "\n",
    "# Convert y_test to a NumPy array if it's a pandas Series\n",
    "y_test_emo = y_test['Emotion'].values if isinstance(y_test['Emotion'], pd.Series) else y_test['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense,LayerNormalization\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Ensure Input Shape\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "# Define a TQDM Progress Bar Callback\n",
    "class TQDMProgressBar(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.epochs_bar = tqdm(total=self.epochs, desc='Training Progress', position=0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs_bar.update(1)\n",
    "        self.epochs_bar.set_postfix(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.epochs_bar.close()\n",
    "\n",
    "# Common Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "embedding_dim=64\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "tqdm_callback = TQDMProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,297,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling1d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sentiment_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │     \u001b[38;5;34m1,297,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m20,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling1d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │        \u001b[38;5;34m10,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sentiment_output (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,334,659</span> (5.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,334,659\u001b[0m (5.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,334,659</span> (5.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,334,659\u001b[0m (5.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Input, Embedding, Conv1D, Dropout,AveragePooling1D, MaxPooling1D, LayerNormalization, LSTM, Bidirectional, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define Sentiment Model\n",
    "sentiment_inputs = Input(shape=(max_length,))\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(sentiment_inputs)\n",
    "\n",
    "# Convolutional layer\n",
    "x = Conv1D(filters=64, kernel_size=5, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Average Pooling layer\n",
    "x = AveragePooling1D(pool_size=2)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Bidirectional LSTM layers\n",
    "x = Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(1e-5)))(x)\n",
    "x = Bidirectional(LSTM(8, return_sequences=True, kernel_regularizer=l2(1e-5)))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# Flatten before Dense layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(16, activation='relu', kernel_regularizer=l2(1e-2))(x)\n",
    "\n",
    "# Output Layer for Sentiment\n",
    "sentiment_output = Dense(3, activation='softmax', name='sentiment_output')(x)\n",
    "\n",
    "# Compile Sentiment Model\n",
    "sentiment_model = Model(inputs=sentiment_inputs, outputs=sentiment_output)\n",
    "sentiment_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "sentiment_model.summary()\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "    monitor='val_loss',        # Monitor validation loss\n",
    "    factor=0.1,                # Reduce learning rate by this factor\n",
    "    patience=5,                # Number of epochs with no improvement to wait\n",
    "    min_lr=1e-6,               # Minimum learning rate\n",
    "    verbose=1                  # Display reduction messages\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.4925 - loss: 1.1670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 1/200 [08:40<28:47:51, 520.96s/it, accuracy=0.557, loss=1.01, val_accuracy=0.672, val_loss=0.752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 100ms/step - accuracy: 0.4925 - loss: 1.1670 - val_accuracy: 0.6718 - val_loss: 0.7517 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7279 - loss: 0.6852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 2/200 [15:56<25:52:36, 470.49s/it, accuracy=0.752, loss=0.64, val_accuracy=0.769, val_loss=0.587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 85ms/step - accuracy: 0.7279 - loss: 0.6852 - val_accuracy: 0.7687 - val_loss: 0.5867 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8291 - loss: 0.4775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 3/200 [23:24<25:12:09, 460.55s/it, accuracy=0.828, loss=0.478, val_accuracy=0.79, val_loss=0.548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 88ms/step - accuracy: 0.8291 - loss: 0.4775 - val_accuracy: 0.7901 - val_loss: 0.5482 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8662 - loss: 0.3896"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 4/200 [31:00<24:58:12, 458.64s/it, accuracy=0.863, loss=0.395, val_accuracy=0.803, val_loss=0.524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 89ms/step - accuracy: 0.8662 - loss: 0.3896 - val_accuracy: 0.8030 - val_loss: 0.5237 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8903 - loss: 0.3318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▎         | 5/200 [38:17<24:25:14, 450.85s/it, accuracy=0.889, loss=0.332, val_accuracy=0.808, val_loss=0.529]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 86ms/step - accuracy: 0.8903 - loss: 0.3318 - val_accuracy: 0.8081 - val_loss: 0.5294 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9088 - loss: 0.2772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 6/200 [45:58<24:29:07, 454.37s/it, accuracy=0.906, loss=0.284, val_accuracy=0.815, val_loss=0.534]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 90ms/step - accuracy: 0.9088 - loss: 0.2772 - val_accuracy: 0.8153 - val_loss: 0.5342 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9241 - loss: 0.2393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 7/200 [54:18<25:09:19, 469.22s/it, accuracy=0.921, loss=0.247, val_accuracy=0.819, val_loss=0.573]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 98ms/step - accuracy: 0.9241 - loss: 0.2393 - val_accuracy: 0.8186 - val_loss: 0.5730 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9357 - loss: 0.2061"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 8/200 [1:02:12<25:06:21, 470.74s/it, accuracy=0.932, loss=0.217, val_accuracy=0.826, val_loss=0.559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 93ms/step - accuracy: 0.9357 - loss: 0.2061 - val_accuracy: 0.8260 - val_loss: 0.5590 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9453 - loss: 0.1835"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 9/200 [1:10:16<25:11:10, 474.72s/it, accuracy=0.942, loss=0.191, val_accuracy=0.821, val_loss=0.618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 89ms/step - accuracy: 0.9453 - loss: 0.1835 - val_accuracy: 0.8207 - val_loss: 0.6178 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9564 - loss: 0.1495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 10/200 [1:17:24<24:18:24, 460.55s/it, accuracy=0.957, loss=0.148, val_accuracy=0.825, val_loss=0.635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 84ms/step - accuracy: 0.9564 - loss: 0.1495 - val_accuracy: 0.8252 - val_loss: 0.6346 - learning_rate: 1.0000e-05\n",
      "Epoch 11/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9594 - loss: 0.1419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 11/200 [1:24:35<23:42:00, 451.43s/it, accuracy=0.96, loss=0.142, val_accuracy=0.824, val_loss=0.634] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 84ms/step - accuracy: 0.9594 - loss: 0.1419 - val_accuracy: 0.8242 - val_loss: 0.6341 - learning_rate: 1.0000e-05\n",
      "Epoch 12/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9610 - loss: 0.1363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 12/200 [1:31:45<23:14:15, 444.98s/it, accuracy=0.961, loss=0.136, val_accuracy=0.824, val_loss=0.659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 84ms/step - accuracy: 0.9610 - loss: 0.1363 - val_accuracy: 0.8236 - val_loss: 0.6593 - learning_rate: 1.0000e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9629 - loss: 0.1330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 13/200 [1:39:10<23:06:49, 444.97s/it, accuracy=0.963, loss=0.133, val_accuracy=0.827, val_loss=0.651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 87ms/step - accuracy: 0.9629 - loss: 0.1330 - val_accuracy: 0.8269 - val_loss: 0.6509 - learning_rate: 1.0000e-05\n",
      "Epoch 14/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9644 - loss: 0.1269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 14/200 [1:46:24<22:49:05, 441.64s/it, accuracy=0.964, loss=0.129, val_accuracy=0.827, val_loss=0.645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 85ms/step - accuracy: 0.9644 - loss: 0.1269 - val_accuracy: 0.8271 - val_loss: 0.6446 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 14/200 [1:46:24<23:33:46, 456.06s/it, accuracy=0.964, loss=0.129, val_accuracy=0.827, val_loss=0.645]\n"
     ]
    }
   ],
   "source": [
    "# Train Sentiment Model\n",
    "sentiment_history = sentiment_model.fit(\n",
    "    X_train,\n",
    "    y_train_sent,  # Use Sentiment labels\n",
    "    validation_data=(X_val, y_val_sent),\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, tqdm_callback,reduce_lr_on_plateau],\n",
    "    class_weight=class_weight_dict_sent,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2517 - loss: 1.6543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 1/200 [05:37<18:38:57, 337.37s/it, accuracy=0.31, loss=1.53, val_accuracy=0.425, val_loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 65ms/step - accuracy: 0.2518 - loss: 1.6543 - val_accuracy: 0.4252 - val_loss: 1.3492 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4893 - loss: 1.2350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 2/200 [10:33<17:13:37, 313.22s/it, accuracy=0.535, loss=1.16, val_accuracy=0.618, val_loss=1.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 58ms/step - accuracy: 0.4893 - loss: 1.2349 - val_accuracy: 0.6175 - val_loss: 1.0332 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m5099/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6740 - loss: 0.8874"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 3/200 [14:25<15:07:06, 276.28s/it, accuracy=0.686, loss=0.866, val_accuracy=0.68, val_loss=0.908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 46ms/step - accuracy: 0.6740 - loss: 0.8874 - val_accuracy: 0.6802 - val_loss: 0.9084 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m5099/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7573 - loss: 0.7051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 4/200 [18:17<14:04:36, 258.55s/it, accuracy=0.757, loss=0.705, val_accuracy=0.7, val_loss=0.871] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 45ms/step - accuracy: 0.7573 - loss: 0.7051 - val_accuracy: 0.7002 - val_loss: 0.8706 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m5099/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8022 - loss: 0.5892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▎         | 5/200 [22:06<13:25:21, 247.80s/it, accuracy=0.799, loss=0.598, val_accuracy=0.715, val_loss=0.856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 45ms/step - accuracy: 0.8022 - loss: 0.5892 - val_accuracy: 0.7152 - val_loss: 0.8557 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8294 - loss: 0.5147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 6/200 [26:04<13:10:19, 244.43s/it, accuracy=0.826, loss=0.522, val_accuracy=0.726, val_loss=0.832]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 47ms/step - accuracy: 0.8294 - loss: 0.5147 - val_accuracy: 0.7261 - val_loss: 0.8321 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8531 - loss: 0.4518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 7/200 [29:58<12:56:18, 241.34s/it, accuracy=0.849, loss=0.46, val_accuracy=0.727, val_loss=0.876] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 46ms/step - accuracy: 0.8531 - loss: 0.4518 - val_accuracy: 0.7275 - val_loss: 0.8760 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8696 - loss: 0.4031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 8/200 [33:47<12:39:15, 237.27s/it, accuracy=0.866, loss=0.413, val_accuracy=0.735, val_loss=0.876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 45ms/step - accuracy: 0.8696 - loss: 0.4031 - val_accuracy: 0.7351 - val_loss: 0.8758 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m5099/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8865 - loss: 0.3569"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 9/200 [37:44<12:35:29, 237.33s/it, accuracy=0.88, loss=0.372, val_accuracy=0.739, val_loss=0.876] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 47ms/step - accuracy: 0.8865 - loss: 0.3569 - val_accuracy: 0.7388 - val_loss: 0.8764 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8949 - loss: 0.3272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 10/200 [41:34<12:23:24, 234.76s/it, accuracy=0.891, loss=0.338, val_accuracy=0.742, val_loss=0.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 45ms/step - accuracy: 0.8949 - loss: 0.3272 - val_accuracy: 0.7420 - val_loss: 0.9197 - learning_rate: 1.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9040 - loss: 0.3018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 11/200 [45:46<12:36:55, 240.29s/it, accuracy=0.9, loss=0.312, val_accuracy=0.746, val_loss=0.9]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 50ms/step - accuracy: 0.9040 - loss: 0.3018 - val_accuracy: 0.7464 - val_loss: 0.9005 - learning_rate: 1.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9208 - loss: 0.2560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 12/200 [50:12<12:56:51, 247.93s/it, accuracy=0.921, loss=0.256, val_accuracy=0.75, val_loss=0.952]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 52ms/step - accuracy: 0.9208 - loss: 0.2560 - val_accuracy: 0.7496 - val_loss: 0.9521 - learning_rate: 1.0000e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9241 - loss: 0.2427"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 13/200 [55:54<14:21:29, 276.41s/it, accuracy=0.924, loss=0.245, val_accuracy=0.749, val_loss=0.972]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 56ms/step - accuracy: 0.9241 - loss: 0.2427 - val_accuracy: 0.7492 - val_loss: 0.9723 - learning_rate: 1.0000e-05\n",
      "Epoch 14/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9272 - loss: 0.2358"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 14/200 [1:00:16<14:04:02, 272.27s/it, accuracy=0.926, loss=0.238, val_accuracy=0.749, val_loss=0.984]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 52ms/step - accuracy: 0.9272 - loss: 0.2358 - val_accuracy: 0.7493 - val_loss: 0.9845 - learning_rate: 1.0000e-05\n",
      "Epoch 15/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9283 - loss: 0.2321"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 15/200 [1:04:38<13:49:54, 269.16s/it, accuracy=0.927, loss=0.236, val_accuracy=0.749, val_loss=0.996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 51ms/step - accuracy: 0.9283 - loss: 0.2321 - val_accuracy: 0.7493 - val_loss: 0.9959 - learning_rate: 1.0000e-05\n",
      "Epoch 16/200\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9290 - loss: 0.2302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 16/200 [1:09:02<13:40:06, 267.43s/it, accuracy=0.928, loss=0.23, val_accuracy=0.749, val_loss=1.02]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m5100/5100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 52ms/step - accuracy: 0.9290 - loss: 0.2302 - val_accuracy: 0.7491 - val_loss: 1.0162 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 16/200 [1:09:02<13:13:56, 258.89s/it, accuracy=0.928, loss=0.23, val_accuracy=0.749, val_loss=1.02]\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# Define Emotion Model\n",
    "emotion_inputs = Input(shape=(max_length,))\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(emotion_inputs)\n",
    "# Convolutional layer\n",
    "x = Conv1D(filters=64, kernel_size=5, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "# Average Pooling layer\n",
    "x = AveragePooling1D(pool_size=4)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Bidirectional LSTM layers\n",
    "x = Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(1e-5)))(x)\n",
    "x = Bidirectional(LSTM(8, return_sequences=True, kernel_regularizer=l2(1e-5)))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# Flatten before Dense layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "\n",
    "# Output Layer for Emotion\n",
    "emotion_output = layers.Dense(6, activation='softmax', name='emotion_output')(x)\n",
    "\n",
    "# Compile Emotion Model\n",
    "emotion_model = models.Model(inputs=emotion_inputs, outputs=emotion_output)\n",
    "emotion_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "tqdm_callback = TQDMProgressBar()\n",
    "# Train Emotion Model\n",
    "emotion_history = emotion_model.fit(\n",
    "    X_train,\n",
    "    y_train_emo,  # Use Emotion labels\n",
    "    validation_data=(X_val, y_val_emo),\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, tqdm_callback,reduce_lr_on_plateau],\n",
    "    class_weight=class_weight_dict_emo,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels for sentiment\n",
    "sentiment_predictions = sentiment_model.predict(X_test)\n",
    "sentiment_predicted_labels = np.argmax(sentiment_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels for emotion\n",
    "emotion_predictions = emotion_model.predict(X_test)\n",
    "emotion_predicted_labels = np.argmax(emotion_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7986274509803921\n",
      "F1 Score: 0.8017075316411655\n",
      "Precision: 0.8080247699310676\n",
      "Recall: 0.7986274509803921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# True labels for sentiment\n",
    "sentiment_true_labels = y_test_sent\n",
    "\n",
    "# Ensure the sentiment map matches the number of classes\n",
    "sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Individual metrics\n",
    "sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predicted_labels)\n",
    "sentiment_f1 = f1_score(sentiment_true_labels, sentiment_predicted_labels, average='weighted')\n",
    "sentiment_precision = precision_score(sentiment_true_labels, sentiment_predicted_labels, average='weighted')\n",
    "sentiment_recall = recall_score(sentiment_true_labels, sentiment_predicted_labels, average='weighted')\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {sentiment_accuracy}\")\n",
    "print(f\"F1 Score: {sentiment_f1}\")\n",
    "print(f\"Precision: {sentiment_precision}\")\n",
    "print(f\"Recall: {sentiment_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7204901960784313\n",
      "F1 Score: 0.7214743952699366\n",
      "Precision: 0.7289946818299825\n",
      "Recall: 0.7204901960784313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# True labels for emotion\n",
    "emo_true_labels = y_test_emo\n",
    "\n",
    "# Ensure the emotion map matches the number of classes\n",
    "emotion_mapping = {\n",
    "    'fear': 0,\n",
    "    'Happiness': 1,\n",
    "    'anger': 2,\n",
    "    'joy' : 3,   \n",
    "    'sadness': 4,    \n",
    "    'neutral': 5, \n",
    "}\n",
    "\n",
    "# Individual metrics\n",
    "emo_accuracy = accuracy_score(emo_true_labels, emotion_predicted_labels)\n",
    "emo_f1 = f1_score(emo_true_labels, emotion_predicted_labels, average='weighted')\n",
    "emo_precision = precision_score(emo_true_labels, emotion_predicted_labels, average='weighted')\n",
    "emo_recall = recall_score(emo_true_labels, emotion_predicted_labels, average='weighted')\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {emo_accuracy}\")\n",
    "print(f\"F1 Score: {emo_f1}\")\n",
    "print(f\"Precision: {emo_precision}\")\n",
    "print(f\"Recall: {emo_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7986274509803921\n",
      "F1 Score: 0.8017075316411655\n",
      "Precision: 0.8080247699310676\n",
      "Recall: 0.7986274509803921\n"
     ]
    }
   ],
   "source": [
    "# True labels for sentiment\n",
    "sentiment_true_labels = y_test_sent\n",
    "\n",
    "# Ensure the sentiment map matches the number of classes\n",
    "sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Individual metrics\n",
    "sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predicted_labels)\n",
    "sentiment_f1 = f1_score(sentiment_true_labels, sentiment_predicted_labels, average='weighted')\n",
    "sentiment_precision = precision_score(sentiment_true_labels, sentiment_predicted_labels, average='weighted')\n",
    "sentiment_recall = recall_score(sentiment_true_labels, sentiment_predicted_labels, average='weighted')\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {sentiment_accuracy}\")\n",
    "print(f\"F1 Score: {sentiment_f1}\")\n",
    "print(f\"Precision: {sentiment_precision}\")\n",
    "print(f\"Recall: {sentiment_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Save the model to a .keras file\\nsentiment_model.save(\\'sentiment_model_prototype.keras\\')\\nprint(\"Model saved to \\'sentiment_model_prototype.keras\\'\")'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Save the model to a .keras file\n",
    "sentiment_model.save('sentiment_model_prototype.keras')\n",
    "print(\"Model saved to 'sentiment_model_prototype.keras'\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Save the model to a .keras file\\nemotion_model.save(\\'emotion_model_prototype.keras\\')\\nprint(\"Model saved to \\'emotion_model_prototype.keras\\'\")'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Save the model to a .keras file\n",
    "emotion_model.save('emotion_model_prototype.keras')\n",
    "print(\"Model saved to 'emotion_model_prototype.keras'\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"hate_speech_labeled_data.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if text is a string (to avoid errors with NaNs)\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", '', text)\n",
    "        \n",
    "        # Remove hashtags and @usernames\n",
    "        text = re.sub(r\"(#[\\d\\w\\.]+)\", '', text)\n",
    "        text = re.sub(r\"(@[\\d\\w\\.]+)\", '', text)\n",
    "        \n",
    "        # Remove numbers (optional if numbers are relevant)\n",
    "        text = re.sub(r\"\\d+\", '', text)\n",
    "        \n",
    "        # **Remove underscores**\n",
    "        text = text.replace('_', '')\n",
    "        \n",
    "        # Remove all special characters except valid punctuation (if needed)\n",
    "        text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "        \n",
    "        # Remove non-ASCII characters\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", '', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "\n",
    "        # Tokenization using nltk\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words (if you have a defined stop words list)\n",
    "        # Uncomment and define stop words if needed:\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Return the cleaned and tokenized text\n",
    "        return tokens\n",
    "    \n",
    "    return text  # Return the input as is if it's not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the 'tweet' column\n",
    "df['tweet'] = df['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80% for training and 20% for temporary set (which will be split into validation and test sets)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df[\"tweet\"], df['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the 20% temporary set into 10% validation and 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for sentiments to unique integer codes\n",
    "hate_mapping = {\n",
    "    'hate': 0,\n",
    "    'offensive': 1,\n",
    "    'neither': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(df[\"tweet\"])\n",
    "# Extract word index and vocabulary size\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "#Convert text into numerical values \n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "#calculate the max length of text\n",
    "max_length = 0\n",
    "for sequence in df['tweet']:\n",
    "    sequence_length = len(sequence)\n",
    "    if sequence_length > max_length:\n",
    "        max_length = sequence_length\n",
    "\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding zeros at the beginning\n",
    "X_train = pad_sequences(X_train,maxlen=max_length)\n",
    "X_val = pad_sequences(X_val,maxlen=max_length)\n",
    "X_test = pad_sequences(X_test,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "#Calculate the vocabulary size\n",
    "vocab_size = len(word_index) + 1\n",
    "class TQDMProgressBar(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.epochs_bar = tqdm(total=self.epochs, desc='Training Progress', position=0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs_bar.update(1)\n",
    "        self.epochs_bar.set_postfix(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.epochs_bar.close()\n",
    "# Learning Rate Scheduler\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "    monitor='val_loss',        # Monitor validation loss\n",
    "    factor=0.1,                # Reduce learning rate by this factor\n",
    "    patience=5,                # Number of epochs with no improvement to wait\n",
    "    min_lr=1e-6,               # Minimum learning rate\n",
    "    verbose=1                  # Display reduction messages\n",
    ")\n",
    "# Common Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "embedding_dim = 32\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "tqdm_callback = TQDMProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5.776923076923077, 1: 0.43048462741010945, 2: 1.9843862599087196}\n"
     ]
    }
   ],
   "source": [
    "# Calculate balanced class weights to handle class imbalance in a dataset.\n",
    "class_weights_hate_speech = compute_class_weight(class_weight='balanced', classes=np.unique(df['class']), y=df['class'])\n",
    "\n",
    "# Convert weights to a dictionary format\n",
    "class_weight_dict_hate_speech = dict(enumerate(class_weights_hate_speech))\n",
    "\n",
    "# Print the class weight dictionary\n",
    "print(class_weight_dict_hate_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to a NumPy array if it's a pandas Series\n",
    "y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "# Convert y_val to a NumPy array if it's a pandas Series\n",
    "y_val = y_val.values if isinstance(y_val, pd.Series) else y_val\n",
    "\n",
    "# Convert y_test to a NumPy array if it's a pandas Series\n",
    "y_test = y_test.values if isinstance(y_test, pd.Series) else y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Ensure Input Shape\\nX_train = np.expand_dims(X_train, axis=-1)\\nX_val = np.expand_dims(X_val, axis=-1)\\nX_test = np.expand_dims(X_test, axis=-1)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Ensure Input Shape\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24783]\n",
      "[24783]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(df[\"tweet\"].shape))\n",
    "print(np.array(df['class'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">648,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hate_speech_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │       \u001b[38;5;34m648,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m6,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hate_speech_output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">664,643</span> (2.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m664,643\u001b[0m (2.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">664,643</span> (2.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m664,643\u001b[0m (2.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3043 - loss: 1.1064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 1/200 [00:29<1:37:00, 29.25s/it, accuracy=0.313, loss=1.11, val_accuracy=0.469, val_loss=1.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 18ms/step - accuracy: 0.3044 - loss: 1.1064 - val_accuracy: 0.4693 - val_loss: 1.0967 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5088 - loss: 1.0280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 2/200 [00:50<1:21:18, 24.64s/it, accuracy=0.597, loss=0.971, val_accuracy=0.726, val_loss=0.769]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.5090 - loss: 1.0279 - val_accuracy: 0.7260 - val_loss: 0.7686 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m1239/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7079 - loss: 0.7893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 3/200 [01:12<1:16:14, 23.22s/it, accuracy=0.704, loss=0.761, val_accuracy=0.738, val_loss=0.73] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 0.7079 - loss: 0.7893 - val_accuracy: 0.7381 - val_loss: 0.7301 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7444 - loss: 0.6427"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 4/200 [01:38<1:19:35, 24.36s/it, accuracy=0.734, loss=0.637, val_accuracy=0.72, val_loss=0.71] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.7444 - loss: 0.6427 - val_accuracy: 0.7195 - val_loss: 0.7103 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7869 - loss: 0.5283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▎         | 5/200 [02:09<1:26:40, 26.67s/it, accuracy=0.785, loss=0.522, val_accuracy=0.772, val_loss=0.628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.7869 - loss: 0.5283 - val_accuracy: 0.7724 - val_loss: 0.6278 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8168 - loss: 0.4413"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 6/200 [02:36<1:27:10, 26.96s/it, accuracy=0.814, loss=0.436, val_accuracy=0.746, val_loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.8168 - loss: 0.4413 - val_accuracy: 0.7458 - val_loss: 0.6814 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m1239/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8445 - loss: 0.3782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 7/200 [03:03<1:26:30, 26.89s/it, accuracy=0.847, loss=0.369, val_accuracy=0.772, val_loss=0.637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8445 - loss: 0.3781 - val_accuracy: 0.7716 - val_loss: 0.6368 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8611 - loss: 0.3274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 8/200 [03:29<1:25:41, 26.78s/it, accuracy=0.861, loss=0.322, val_accuracy=0.778, val_loss=0.63] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 21ms/step - accuracy: 0.8611 - loss: 0.3274 - val_accuracy: 0.7780 - val_loss: 0.6300 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8799 - loss: 0.2659"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 9/200 [03:52<1:21:22, 25.57s/it, accuracy=0.877, loss=0.272, val_accuracy=0.761, val_loss=0.691]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 0.8799 - loss: 0.2659 - val_accuracy: 0.7611 - val_loss: 0.6907 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m1239/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8976 - loss: 0.2297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 10/200 [04:18<1:21:22, 25.70s/it, accuracy=0.895, loss=0.24, val_accuracy=0.758, val_loss=0.717] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8976 - loss: 0.2297 - val_accuracy: 0.7579 - val_loss: 0.7169 - learning_rate: 1.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9083 - loss: 0.1927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 11/200 [04:47<1:24:16, 26.75s/it, accuracy=0.908, loss=0.197, val_accuracy=0.774, val_loss=0.694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9083 - loss: 0.1927 - val_accuracy: 0.7744 - val_loss: 0.6936 - learning_rate: 1.0000e-05\n",
      "Epoch 12/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9121 - loss: 0.1854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 12/200 [05:17<1:26:04, 27.47s/it, accuracy=0.912, loss=0.194, val_accuracy=0.78, val_loss=0.692] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9121 - loss: 0.1854 - val_accuracy: 0.7801 - val_loss: 0.6921 - learning_rate: 1.0000e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m1239/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9154 - loss: 0.1853"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 13/200 [05:47<1:28:27, 28.38s/it, accuracy=0.916, loss=0.186, val_accuracy=0.771, val_loss=0.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 25ms/step - accuracy: 0.9154 - loss: 0.1853 - val_accuracy: 0.7708 - val_loss: 0.7203 - learning_rate: 1.0000e-05\n",
      "Epoch 14/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9129 - loss: 0.1799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 14/200 [06:16<1:28:39, 28.60s/it, accuracy=0.915, loss=0.182, val_accuracy=0.788, val_loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9129 - loss: 0.1799 - val_accuracy: 0.7881 - val_loss: 0.6952 - learning_rate: 1.0000e-05\n",
      "Epoch 15/200\n",
      "\u001b[1m1238/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9199 - loss: 0.1739"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 15/200 [06:45<1:28:27, 28.69s/it, accuracy=0.919, loss=0.178, val_accuracy=0.776, val_loss=0.711]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m1240/1240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9199 - loss: 0.1739 - val_accuracy: 0.7764 - val_loss: 0.7112 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 15/200 [06:45<1:23:21, 27.04s/it, accuracy=0.919, loss=0.178, val_accuracy=0.776, val_loss=0.711]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Embedding, Conv1D, Dropout,AveragePooling1D, MaxPooling1D, LayerNormalization, LSTM, Bidirectional, Dense\n",
    "# Define hate/offensive Model\n",
    "hate_speech_inputs = Input(shape=(max_length,))\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(hate_speech_inputs)\n",
    "# Convolutional layer\n",
    "x = Conv1D(filters=32, kernel_size=3, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Max Pooling layer\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# LSTM layers\n",
    "x = Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(1e-5)))(x)\n",
    "x = Bidirectional(LSTM(8, return_sequences=True, kernel_regularizer=l2(1e-5)))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# Flatten before Dense layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(16, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "\n",
    "# Output Layer for hate/offensive\n",
    "hate_speech_output = layers.Dense(3, activation='softmax', name='hate_speech_output')(x)\n",
    "\n",
    "# Compile hate/offensive Model\n",
    "hate_speech_model = models.Model(inputs=hate_speech_inputs, outputs=hate_speech_output)\n",
    "hate_speech_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "hate_speech_model.summary()\n",
    "# Train hate/offensive Model\n",
    "hate_speech_history = hate_speech_model.fit(\n",
    "    X_train,\n",
    "    y_train,  \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, tqdm_callback,reduce_lr_on_plateau],\n",
    "    class_weight=class_weight_dict_hate_speech,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'hate_speech_model_prototype.keras'\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a .keras file\n",
    "hate_speech_model.save('hate_speech_model_prototype.keras')\n",
    "print(\"Model saved to 'hate_speech_model_prototype.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step\n",
      "Accuracy: 0.7676482452601856\n",
      "F1 Score: 0.7947524059682408\n",
      "Precision: 0.8381895166367666\n",
      "Recall: 0.7676482452601856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "# Predict class labels for hate/offensive\n",
    "hate_predictions = hate_speech_model.predict(X_test)\n",
    "hate_predicted_labels = np.argmax(hate_predictions, axis=1)\n",
    "# True labels for hate/offensive\n",
    "hate_true_labels =y_test\n",
    "# Define a mapping for hate/offensive to unique integer codes\n",
    "hate_mapping = {\n",
    "    'hate': 0,\n",
    "    'offensive': 1,\n",
    "    'neither': 2\n",
    "}\n",
    "\n",
    "# Individual metrics\n",
    "hate_accuracy = accuracy_score(hate_predicted_labels,hate_true_labels)\n",
    "hate_f1 = f1_score(hate_true_labels, hate_predicted_labels, average='weighted')\n",
    "hate_precision = precision_score(hate_true_labels, hate_predicted_labels, average='weighted')\n",
    "hate_recall = recall_score(hate_true_labels, hate_predicted_labels, average='weighted')\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {hate_accuracy}\")\n",
    "print(f\"F1 Score: {hate_f1}\")\n",
    "print(f\"Precision: {hate_precision}\")\n",
    "print(f\"Recall: {hate_recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
